{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO30RH4bNqCZpJzKzkmoCvN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChintPatel/CMPE258-HW8/blob/main/HW9_Transformers_and_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mo6z98SfJ1DU",
        "outputId": "bc687f5f-7cca-4286-9cac-5a90954e062b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --quiet tensorflow tensorflow-text tensorflow-hub transformers datasets keras-nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Inference with a Pretrained Classifier"
      ],
      "metadata": {
        "id": "G-nsoJsfKsIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# ‚Äî Text Generation (GPT-2) ‚Äî\n",
        "gen = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "print(\"=== Text Generation ===\")\n",
        "print(gen(\"In a world where AI rules,\", max_length=50, do_sample=True)[0][\"generated_text\"])\n",
        "\n",
        "# ‚Äî Sentiment Analysis (DistilBERT) ‚Äî\n",
        "clf = pipeline(\"sentiment-analysis\")\n",
        "print(\"\\n=== Sentiment Analysis ===\")\n",
        "for txt in [\"I love this movie!\", \"This was the worst book I‚Äôve read.\"]:\n",
        "    print(f\"{txt} ->\", clf(txt)[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPtNSC9EKF9Z",
        "outputId": "79a833d1-0385-4f5f-8139-b006dd6fb67c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Text Generation ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In a world where AI rules, you'd be forgiven for thinking this scenario isn't realistic. There are still big, powerful AI agents, and some of the most well-known are:\n",
            "\n",
            "The US Department of Justice (DOJ) The\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Sentiment Analysis ===\n",
            "I love this movie! -> {'label': 'POSITIVE', 'score': 0.9998775720596313}\n",
            "This was the worst book I‚Äôve read. -> {'label': 'NEGATIVE', 'score': 0.9997784495353699}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Fine-tuning a Pretrained Backbone (DistilBERT ‚Üí IMDb)"
      ],
      "metadata": {
        "id": "wvSreoZ_KvEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Load IMDb via TensorFlow Datasets instead of ü§ódatasets\n",
        "import tensorflow_datasets as tfds\n",
        "from transformers import DistilBertTokenizerFast\n",
        "\n",
        "# 1. Download & split\n",
        "(raw_train, raw_test), ds_info = tfds.load(\n",
        "    'imdb_reviews',\n",
        "    split=['train', 'test'],\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True,\n",
        "    with_info=True\n",
        ")\n",
        "\n",
        "# 2. Prepare tokenizer\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "max_length = 128\n",
        "\n",
        "print(f\"‚úÖ Loaded IMDb: {ds_info.splits['train'].num_examples} train, \"\n",
        "      f\"{ds_info.splits['test'].num_examples} test\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXUgJ4y6LGiI",
        "outputId": "6b72627f-fbee-4f66-da75-4e224e0d4741"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded IMDb: 25000 train, 25000 test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build & Train Your Own Transformer from Scratch"
      ],
      "metadata": {
        "id": "LAoPQnPCK6IF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# 1. Load & split IMDb from TFDS\n",
        "all_train = tfds.load('imdb_reviews', split='train', as_supervised=True)\n",
        "all_train = all_train.shuffle(10_000, seed=42)\n",
        "train_ds  = all_train.take(5000)\n",
        "val_ds    = all_train.skip(5000).take(2000)\n",
        "\n",
        "# 2. Text Vectorization layer\n",
        "max_features  = 20_000\n",
        "sequence_len  = 200\n",
        "vectorize = layers.TextVectorization(\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_len\n",
        ")\n",
        "# Adapt on the text-only stream\n",
        "vectorize.adapt(train_ds.map(lambda text, label: text))\n",
        "\n",
        "# 3. Define a single Transformer block\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.attn       = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn        = keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation='relu'),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.ln1        = layers.LayerNormalization()\n",
        "        self.ln2        = layers.LayerNormalization()\n",
        "        self.drop1      = layers.Dropout(rate)\n",
        "        self.drop2      = layers.Dropout(rate)\n",
        "\n",
        "    # Modified call method to accept *args and **kwargs\n",
        "    def call(self, inputs, training=None, *args, **kwargs):\n",
        "        attn_out = self.attn(inputs, inputs)\n",
        "        # Pass training to Dropout layers explicitly\n",
        "        attn_out = self.drop1(attn_out, training=training)\n",
        "        out1     = self.ln1(inputs + attn_out)\n",
        "        ffn_out  = self.ffn(out1)\n",
        "        # Pass training to Dropout layers explicitly\n",
        "        ffn_out  = self.drop2(ffn_out, training=training)\n",
        "        return self.ln2(out1 + ffn_out)\n",
        "\n",
        "# 4. Build the model\n",
        "embed_dim = 64\n",
        "num_heads = 2\n",
        "ff_dim    = 64\n",
        "\n",
        "inputs = layers.Input(shape=(sequence_len,), dtype='int64')\n",
        "x = layers.Embedding(max_features, embed_dim)(inputs)\n",
        "# The functional API should pass 'training' implicitly now\n",
        "x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x) # This Dropout layer will receive 'training' implicitly\n",
        "x = layers.Dense(20, activation='relu')(x)\n",
        "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "scratch_model = keras.Model(inputs, outputs)\n",
        "scratch_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 5. Prepare tf.data pipelines\n",
        "def prepare(ds):\n",
        "    return (\n",
        "        ds\n",
        "        .map(lambda text, label: (vectorize(text), label),\n",
        "             num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        .batch(32)\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "    )\n",
        "\n",
        "tf_train = prepare(train_ds)\n",
        "tf_val   = prepare(val_ds)\n",
        "\n",
        "# 6. Train!\n",
        "scratch_model.fit(\n",
        "    tf_train,\n",
        "    validation_data=tf_val,\n",
        "    epochs=3\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rM9QXx-na-J1",
        "outputId": "97c50b3f-8267-4c1b-fa76-0da8f35a0c51"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 242ms/step - accuracy: 0.5730 - loss: 0.6813 - val_accuracy: 0.7705 - val_loss: 0.4517\n",
            "Epoch 2/3\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 255ms/step - accuracy: 0.8368 - loss: 0.3728 - val_accuracy: 0.8755 - val_loss: 0.3188\n",
            "Epoch 3/3\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 223ms/step - accuracy: 0.8991 - loss: 0.2549 - val_accuracy: 0.9080 - val_loss: 0.2289\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7df88aa886d0>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ]
}